# deep_nlp_on_sf_literature

**Note**: This project is still under development, in its final stages. However, most individual modules are complete and can be used or repurposed. The documentation is also mostly accurate and complete. However, please know that some of the organization and in-project README.txt files have not been updated yet. For example, the README.txt in the "Archive" directory does not really capture how the directory has evolved since then. 

As of this writing, I am currently in the process of fine-tuning a RoBERTa model with the output acquired from `GPT_NER_Round_1.py`, in order to perform last-stage NER using RoBERTa. If you wish to begin exploring, I suggest beginning with `CorpusProcessor_with_NER.py` in the `main files` directory (which actually does not contain any NER implementation in itself), where you will find code for multifarious data preprocessing. If you wish to run it as a script, however, please set the `FILEPATH` variable to a file you have access to and want to process. I have not yet hosted `internet_archive_scifi_v3.txt` (the corpus I was using) on GitHub thanks to its size.

**Overview**

Most of the action as of the current version is contained in the directories "main files", "TF-IDF", and "LLM". In "main files", begin with `CorpusProcessor_with_NER.py` that was used for preprocessing the corpus. Then go to `NER.py` that performs first-stage NER on the corpus using [spaCy](https://github.com/explosion/spaCy) and stores the output in `NER_output2.csv` by default.

After performing basic NER, you can check out `sentences_of_entities.py`. If you run it as a script, it (attempts to) extract the famed 3.5M sentences from `sci-fi_text_cleaned.csv`, the csv file representing a dataframe that contains the corpus as a list of sentences with their cleaned versions. Then from this list, `sentences_of_entities` extracts all the sentences that contain any of the entities that were found by spaCy's NER. (Note that these entities are brought in from `entity_list.txt` which is a file I created using a separate script, after running `NER.py`. `entity_list.txt` _is_ small enough to be included in the "main files" directory, so it is. Also note that in the current version of the code, `sci-fi_text_cleaned.csv` has not been added already to "main files". You must create this file or slightly modify the code in `sentences_of_entities` to have it work as a script)

Now the results of `sentences_of_entities.py` will be stored in the "LLM" directory, in `sents_containing_named_entities.txt`. But don't go to the `LLM` directory just yet: first go to "TF-IDF" and run `TF-IDF.py` as a script. This will use [scikit-learn](https://github.com/scikit-learn/scikit-learn)'s TfidfVectorizer and KMeans clustering to extract a **diverse** batch of 8000 sentences from `sents_containing_named_entities.txt`. The diverse batch of sentences will be stored in `representative_sents_for_GPT_labelling.txt` that remains in the `TF-IDF` directory.

Finally, you can now go to the "LLM" directory and run `GPT_NER_round_1.py` and run it as a script. I have already created the data you can use, so when asked about it you can any key except "Y" to generate RoBERTa training data. If you want to see real magic however, first go to `api_key.py` and insert a valid OpenAI API key.
